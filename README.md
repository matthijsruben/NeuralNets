# NeuralNets
This repo contains code for several different neural networks. This project is created by Matthijs Kok and is used for a [bachelor thesis](#final_research_paper_matthijs_kok.pdf) of the study Business and IT on the University of Twente. In order to use/run code of some files in this repo, make sure you adhere to the [requirements](#requirements). For an [explanation](#explanation-of-the-files-and-folders) you can read the descriptions of the files below. For help, questions or remarks, please send an email to m.r.kok@student.utwente.nl. 

P.S. Note that the folder TFNNs contains personal Neural Network tryouts that were created using libraries such as Tensorflow and Keras. This folder and its contents are not used for the bachelor project.

# Requirements:
There are a few things required, before you will be able to run the code published on this repository. A 64-bit Python version 3.5-3.7 is required, because of compatibility reasons with Tensorflow (library). The code was written in Python 3.6.2. and makes use of the following libraries: tensorflow, keras, datetime, random, gzip, numpy, matplotlib. Only the last four libraries are needed to run the files in the root directory (ANNScratch.py, ExampleNN.py, MNISTloader.py). The first three libraries are also needed to run the code in the folder TFNNs, which contains networks that make use of tensorflow.

---

# Explanation of the files and folders:

## Final_Research_Paper_Matthijs_Kok.pdf
This is a pdf file of the research paper of this project. All the code on this repo (except the folder TFNNs) is used for this project.

## MNIST_data
This folder contains the data of the dataset MNIST by LeCun, Y. et al.(http://yann.lecun.com/exdb/mnist/). It contains four compressed files. train_images-dx3-ubyte.gz and train-labels-idx1-ubyte.gz contain the training set images and training set labels, respectively. t10k-images-idx3-ubyte.gz, t10k-labels-idx1-ubyte.gz contain test set images and test set labels respectively.

## plots
This folder contains 8 images of plots that were created using matplotlib by plotting the data of a network that was training. The file AccuracySGD.png shows the accuracy (number of correct predictions / total number of predictions) of the network per epoch in the training session. The file LossSGD.png shows the value of the loss function averaged over all training examples every epoch. newCompetitiveAccuracyHPT.png shows the accuracy of the HPT-Competitive method for 20 epochs and for 6 different values of the influence parameter c. newCompetitiveLossHPT.png shows the loss of the HPT-Competitive method for 20 epochs and for 6 different values of the influence parameter c. newImplyHPT.png shows the accuracy of the HPT-Imply method for 20 epochs and for 6 different values of the influence parameter c. newImplyLossHPT.png shows the loss of the HPT-Imply method for 20 epochs and for 6 different values of the influence parameter c. observedSynapticChangeHPTSGD4layersColors.png and observedSynapticChangeSGD4layersColors.png both show a scatterplot where each dot represents a weight update (red = positive, blue = negative) for the weight of the connection between the first neuron of the second layer and the first neuron of the third layer. The weight update (called 'synaptic change' in the graph) is plotted against the values of the activations of the pre- and post-synaptic neuron. observedSynapticChangeHPTSGD4layersColors.png shows the synaptic changes of the HPT method over 5 epochs and observedSynapticChangeSGD4layersColors.png shows the synaptic changes of standard SGD over 5 epochs.

Similar plots could be generated by running (and uncommenting some parts of) ExampleNN.py.

## ExampleNN.py
This file makes use of both MNISTloader.py and ANNScratchMNIST.py to create an instance of a network (specified in ANNScratchMNIST.py) that can be trained on the MNIST_data that is loaded and formatedd in MNISTloader.py. ExampleNN.py trains a network, with 4 layers containing 784, 30, 30, 10 neurons (ordered from first to last layer), with stochastic gradient descent. The network is trained in 20 epochs, mini-batches of 28 training examples, and a learning rate of 0.5. Furthermore, a network is trained with the HPT method using the same benchmark network. Networks where hyperparameters are changed or code that generates graphs used for the paper are commented out.

## MNISTloader.py
This file is used to read, decompress and format the data from the MNIST dataset. It makes use of gzip to decompress and read the file as a stream of bytes. Numpy is used to convert the array of bytes into an array of integers. The images and labels are returned by the method load_data(train_or_val) as a list of column vectors containing 784 (28x28) and 10 entries, respectively.

## ANNScratchMNIST.py
This file is the most important file in this entire repo. It describes the way how a neural network and its training method is defined. The file got its name from the fact that it was created from scratch, without the use of libraries that pre-implemented parts of a neural network or its training method, like tensorflow and keras. The structure of this file was inspired by the description of Michael A. Nielsen in his book "Neural Networks and Deep Learning" (https://github.com/mnielsen/neural-networks-and-deep-learning.git) and the python 3 version of this code (https://github.com/MichalDanielDobrzanski/DeepLearningPython35.git) that was provided by Michal D. Dobrzanski. Some parts of the code are quite similar, such as the constructor of the network (init function), however many parts are implemented very differently, such as the SGD (stochastic gradient descent) function as well as the functions called from this function. The reason this part is different is not a lack of trust in those sources, but rather that I wanted to fully understand the mathematics of backpropagation myself.

## TFNNs
This folder contains 2 neural networks that both make use of the libraries tensorflow and keras (and datetime). The neural network programmed in ANNFashionMNIST.py trains on the fashionMNIST dataset provided by keras. The neural network programmed in ANNMNIST.py trains on the MNIST dataset provided by keras. It also contains a folder logs which is used to deposit data generated by running the neural network in ANNFashionMNIST.py. The files in TFNNs were used to get an idea of programming neural networks using tensorflow and keras, but the folder not used any further in this project. The files and folders in this folder will be explained below.

#### ANNFashionMNIST.py
The file ANNFashionMNIST.py contains code for a neural network that can be trained on the dataset of fashionMNIST, which contains pictures of different types of clothes.
Short explanation of the code:

First, The data is divided into training data and test (validation) data. Since the data contains 28x28 pixel grayscale images (0-255), their values are first divided by 255, so that the grayscale value will be represented as a number between 0 (white) and 1 (black).

Then, the model of the Neural Network is created. There are 3 layers. The first layer is used for dimensionality reduction, such that the image is converted to a 784-dimensional vector. Each component of this vector will be used as input unit within the layer. The second layer contains 128 units which are activated using the 'relu' activation function. The third layer contains 10 units corresponding to the amount of classes (10 different cloths), which are activated using the 'softmax' activation function in order to acquire a probability used by the network to predict which type of cloth it think it is.

Next, the model is compiled using the optimizer 'adam', because 'adam' nicely incorporates both 'SGD with momentum' and 'RMSProp'. 'Momentum' uses the exponentially weighted moving average of a time series of gradients in order to accelerate the approach to the (local) minimum of the loss function and to dampen the oscillating effect in pathological curvatures on the surface of the loss function, such as ravines. 'RMSProp' automatically performs simulated annealing in the learning parameter, such that weight update steps become smaller when approaching a minimum. 'Adam' nicely combines the two, which is the reason it is chosen. Currently, 'sparse_categorical_crossentropy' is chosen as the loss function, which is a rather arbitrary choice. Also, the accuracy metric, which calculates how often prediction equals lables (actually it calculates the fraction of correct predictions), is specified in order to keep track of its value each epoch.

Lastly, the training is started, where 5 epochs are specified (which is a rather low amount of epochs).

Finally, the first 5 images from the test (validation) data are visualised as an image, where both the label and prediciton of the network are displayed.

#### ANNMNIST.py
The file ANNMNIST.py contains code for a neural network that can be trained on the dataset MNIST, which contains pictures of handwritten digits.
Short explanation of the code:

First, The data is divided into training data and test (validation) data. Since the data contains 28x28 pixel grayscale images (0-255), their values are first divided by 255, so that the grayscale value will be represented as a number between 0 (white) and 1 (black).

Then, the model of the Neural Network is created. There are 3 layers. The first layer is used for dimensionality reduction, such that the image is converted to a 784-dimensional vector. Each component of this vector will be used as input unit within the layer. The second layer contains 64 units which are activated using the 'sigmoid' activation function. The last layer is a dense layer with 10 units, which are activated using the 'softmax' activation function in order to acquire a probability used by the network to predict the type of digit.

Next, the model is compiled using the optimizer 'SGD' (stochastic gradient descent) with a parameter of 0.5 (which is the learning rate). Currently, 'sparse_categorical_crossentropy' is chosen as the loss function, because it appears to outperform the mean squared error loss function significantly. Also, the accuracy metric, which calculates how often prediction equals lables (actually it calculates the fraction of correct predictions), is specified in order to keep track of its value each epoch.

Right before the model starts to train, the tensorboard callback is defined. This causes the model to log metrics while training when specifying the callback in the model.fit function. These metrics can be found in graph-form, if you follow the guide described in [logs](#logs)

#### logs
The logs is a folder which stores files with data that was acquired while training a neural network. They can be opened by opening your command line in the folder of this project on your local repo, typing: 'tensorboard --logdir=PATH-TO-LOG' (e.g. 'tensorboard --logdir=logs\fit\20200518-223753'). While this is running, open your browser on localhost:6006 and a dashboard generated by tensorboard with statistics should appear. If not, the output of your command window probably told you it is on another port (other than 6006).
